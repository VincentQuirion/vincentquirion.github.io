---
---

@inproceedings{quirion2024scaling,
  title={Scaling Laws for Language Model Reasoning},
  author={Quirion, Vincent and Smith, Alice and Johnson, Bob},
  booktitle={International Conference on Machine Learning},
  year={2024},
  description={We study how reasoning capabilities in language models scale with compute, data, and model size. We find predictable scaling laws that can guide efficient training.},
  arxiv={2401.12345},
  code={https://github.com/example/scaling-reasoning},
  selected={true}
}

@article{quirion2023mechanistic,
  title={Mechanistic Interpretability of In-Context Learning},
  author={Quirion, Vincent and Chen, Wei and Patel, Priya},
  journal={Advances in Neural Information Processing Systems},
  year={2023},
  description={We reverse-engineer the circuits responsible for in-context learning in transformers, identifying key attention patterns and MLP computations.},
  arxiv={2312.54321},
  code={https://github.com/example/icl-interp},
  selected={true}
}

@inproceedings{quirion2023alignment,
  title={Constitutional AI: Harmlessness from AI Feedback},
  author={Quirion, Vincent and Lee, Sarah and Kumar, Raj},
  booktitle={Association for Computational Linguistics},
  year={2023},
  description={We present a method for training AI assistants to be helpful, harmless, and honest using AI-generated feedback rather than human labels.},
  pdf={https://arxiv.org/pdf/2212.08073.pdf},
  website={https://example.com/constitutional-ai},
  selected={true}
}

@article{quirion2022transformers,
  title={Emergent Abilities of Large Language Models},
  author={Quirion, Vincent and Anderson, James and Williams, Emma},
  journal={Transactions on Machine Learning Research},
  year={2022},
  description={We document emergent abilities—capabilities that appear suddenly at scale—in large language models, including arithmetic, multilingual translation, and reasoning.},
  arxiv={2206.07682},
  selected={true}
}
